{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5207fc0f-20ab-4503-9df7-6b4cc29877fb",
      "metadata": {
        "id": "5207fc0f-20ab-4503-9df7-6b4cc29877fb"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "#path2data_train = \"/root/COCO/train2017\"\n",
        "#path2json_train = \"/root/COCO/annotations/instances_train2017.json\"\n",
        "path2json_val = os.path.expanduser(\"~/fiftyone/coco-2017/raw/instances_val2017.json\")\n",
        "path2data_val = \"/home/jupyter-st125053/fiftyone/coco-2017/validation\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rFZLMLBct4_",
        "outputId": "e43870c3-c9e0-4159-b8dd-4ccc809eb2fa"
      },
      "id": "_rFZLMLBct4_",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fiftyone in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from fiftyone) (24.1.0)\n",
            "Requirement already satisfied: argcomplete in /usr/local/lib/python3.11/dist-packages (from fiftyone) (3.5.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.12.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.36.12)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from fiftyone) (5.5.1)\n",
            "Requirement already satisfied: dacite<1.8.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.7.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.2.18)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from fiftyone) (6.3.1)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.11.0)\n",
            "Requirement already satisfied: hypercorn>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.17.3)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (3.1.5)\n",
            "Requirement already satisfied: kaleido!=0.2.1.post1 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fiftyone) (3.10.0)\n",
            "Requirement already satisfied: mongoengine~=0.29.1 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.29.1)\n",
            "Requirement already satisfied: motor~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fiftyone) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2.2.2)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (11.1.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (5.24.1)\n",
            "Requirement already satisfied: pprintpp in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from fiftyone) (5.9.5)\n",
            "Requirement already satisfied: pymongo~=4.9.2 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.9.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2024.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from fiftyone) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from fiftyone) (2024.11.6)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.3.4)\n",
            "Requirement already satisfied: rtree in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.25.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.13.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from fiftyone) (75.1.0)\n",
            "Requirement already satisfied: sseclient-py<2,>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.8.0)\n",
            "Requirement already satisfied: sse-starlette<1,>=0.10.3 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.10.3)\n",
            "Requirement already satisfied: starlette>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.45.3)\n",
            "Requirement already satisfied: strawberry-graphql in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.258.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.9.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.14.2)\n",
            "Requirement already satisfied: universal-analytics-python3<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.1.1)\n",
            "Requirement already satisfied: pydash in /usr/local/lib/python3.11/dist-packages (from fiftyone) (8.0.5)\n",
            "Requirement already satisfied: fiftyone-brain<0.20,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.19.0)\n",
            "Requirement already satisfied: fiftyone-db<2.0,>=0.4 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (1.1.7)\n",
            "Requirement already satisfied: voxel51-eta<0.15,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from fiftyone) (0.14.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from fiftyone) (4.11.0.86)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.14.0)\n",
            "Requirement already satisfied: h2>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone) (4.2.0)\n",
            "Requirement already satisfied: priority in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.0)\n",
            "Requirement already satisfied: wsproto>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from hypercorn>=0.13.2->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3->fiftyone) (3.0.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.14->fiftyone) (9.0.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo~=4.9.2->fiftyone) (2.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette>=0.24.0->fiftyone) (3.7.1)\n",
            "Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.28.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.3.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.7)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (4.0.0)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.17.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (5.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->fiftyone) (2.6)\n",
            "Requirement already satisfied: botocore<1.37.0,>=1.36.12 in /usr/local/lib/python3.11/dist-packages (from boto3->fiftyone) (1.36.12)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->fiftyone) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->fiftyone) (0.11.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->fiftyone) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->fiftyone) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fiftyone) (3.2.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fiftyone) (2025.1)\n",
            "Requirement already satisfied: typing-extensions!=4.6.0,>3.10 in /usr/local/lib/python3.11/dist-packages (from pydash->fiftyone) (4.12.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->fiftyone) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fiftyone) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fiftyone) (3.5.0)\n",
            "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql->fiftyone) (3.2.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette>=0.24.0->fiftyone) (1.3.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->voxel51-eta<0.15,>=0.14.0->fiftyone) (25.1.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (3.21.0)\n",
            "Requirement already satisfied: pyzstd>=0.15.9 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (0.16.2)\n",
            "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.1.1)\n",
            "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.3)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (0.2.3)\n",
            "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->voxel51-eta<0.15,>=0.14.0->fiftyone) (3.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "617ee4be-9af2-45a1-b46f-f668403e7415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "617ee4be-9af2-45a1-b46f-f668403e7415",
        "outputId": "094056a9-8842-4131-e049-3eb53b63a2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting existing directory '/root/fiftyone/coco-2017/validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Overwriting existing directory '/root/fiftyone/coco-2017/validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████|    6.1Gb/6.1Gb [12.5s elapsed, 0s remaining, 784.6Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    6.1Gb/6.1Gb [12.5s elapsed, 0s remaining, 784.6Mb/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO validation dataset re-downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "destination_path = \"/home/jupyter-st125053/fiftyone/coco-2017/\"\n",
        "\n",
        "# Ensure dataset directory is correctly set\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    dataset_dir=destination_path,\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "print(\"COCO validation dataset re-downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ncVkMMeL1O",
        "outputId": "318c432a-5808-42cf-b3a2-7dc15e49ead8"
      },
      "id": "p3ncVkMMeL1O",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'custom_coco.py', '__pycache__', 'coco_cats.json', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a6ce5d25-c429-4517-97ad-f67bb9b54bcb",
      "metadata": {
        "id": "a6ce5d25-c429-4517-97ad-f67bb9b54bcb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from custom_coco import CustomCoco"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone.zoo as foz\n",
        "\n",
        "dataset_dir = \"/home/jupyter-st125053/fiftyone/coco-2017/\"\n",
        "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\", dataset_dir=dataset_dir, overwrite=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQB1yG18e0VZ",
        "outputId": "130ded2c-0e91-40ed-d77b-ef7ea651e669"
      },
      "id": "sQB1yG18e0VZ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting existing directory '/root/fiftyone/coco-2017/validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Overwriting existing directory '/root/fiftyone/coco-2017/validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Downloading images to '/root/fiftyone/coco-2017/tmp-download/val2017.zip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████|    6.1Gb/6.1Gb [8.3s elapsed, 0s remaining, 772.9Mb/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    6.1Gb/6.1Gb [8.3s elapsed, 0s remaining, 772.9Mb/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Extracting images to '/root/fiftyone/coco-2017/validation/data'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f076ea4a-0662-4a42-b683-ba289d52c958",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f076ea4a-0662-4a42-b683-ba289d52c958",
        "outputId": "c251761a-6c5f-4623-d56f-8c4d03f572c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.10s)\n",
            "creating index...\n",
            "index created!\n",
            "✅ Dataset cleaned! 0 valid images copied to /home/jupyter-st125053/fiftyone/coco-2017/validation\n"
          ]
        }
      ],
      "source": [
        "# ✅ Backup and clean the directory\n",
        "if os.path.exists(path2data_val):\n",
        "    shutil.rmtree(path2data_val)  # Remove broken dataset\n",
        "os.makedirs(path2data_val, exist_ok=True)  # Recreate the directory\n",
        "\n",
        "# ✅ Load dataset and filter valid images\n",
        "dataset = CustomCoco(root=path2data_val, annFile=path2json_val)\n",
        "\n",
        "valid_image_count = 0\n",
        "for img_id in dataset.coco.imgs.keys():\n",
        "    img_info = dataset.coco.loadImgs(img_id)[0]\n",
        "    img_path = os.path.join(path2data_val, img_info['file_name'])\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        valid_image_count += 1\n",
        "        shutil.copy(img_path, path2data_val)  # Copy only valid images\n",
        "\n",
        "print(f\"✅ Dataset cleaned! {valid_image_count} valid images copied to {path2data_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c9b74006-bbdc-4b36-8c1a-d2204fea1dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9b74006-bbdc-4b36-8c1a-d2204fea1dcd",
        "outputId": "2a35d8d9-b823-467e-a9ec-53ae65a3cc32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Image is missing. You need to re-download the dataset.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path_to_image = \"/home/jupyter-st125053/fiftyone/coco-2017/validation/000000000139.jpg\"\n",
        "\n",
        "if os.path.exists(path_to_image):\n",
        "    print(\"✅ Image exists!\")\n",
        "else:\n",
        "    print(\"❌ Image is missing. You need to re-download the dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())  # Check the current working directory\n",
        "print(os.listdir(\".\"))  # List files in the current directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_-c34OqjZH7",
        "outputId": "9ecb954c-fb30-49fb-d032-add08cdf6a88"
      },
      "id": "i_-c34OqjZH7",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'custom_coco.py', 'darknet.py', 'full_train_yolov4.py', '__pycache__', 'util.py', 'mish.py', 'coco_cats.json', 'darknet_test.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep \"class Darknet\" darknet.py\n"
      ],
      "metadata": {
        "id": "XMQQngQpj2en"
      },
      "id": "XMQQngQpj2en",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat darknet.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS51Uualjnoo",
        "outputId": "7fa41827-60cd-4b28-dc3c-5b223ed3db0b"
      },
      "id": "FS51Uualjnoo",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from __future__ import division\n",
            "\n",
            "import torch \n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F \n",
            "from torch.autograd import Variable\n",
            "import numpy as np\n",
            "from mish import Mish\n",
            "from util import * \n",
            "\n",
            "\n",
            "\n",
            "def get_test_input():\n",
            "    img = cv2.imread(\"dog-cycle-car.png\")\n",
            "    img = cv2.resize(img, (608,608))          #Resize to the input dimension\n",
            "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
            "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
            "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
            "    img_ = Variable(img_)                     # Convert to Variable\n",
            "    return img_\n",
            "\n",
            "def parse_cfg(cfgfile):\n",
            "    \"\"\"\n",
            "    Takes a configuration file\n",
            "    \n",
            "    Returns a list of blocks. Each blocks describes a block in the neural\n",
            "    network to be built. Block is represented as a dictionary in the list\n",
            "    \n",
            "    \"\"\"\n",
            "    \n",
            "    file = open(cfgfile, 'r')\n",
            "    lines = file.read().split('\\n')                        # store the lines in a list\n",
            "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
            "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
            "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
            "    \n",
            "    block = {}\n",
            "    blocks = []\n",
            "    \n",
            "    for line in lines:\n",
            "        if line[0] == \"[\":               # This marks the start of a new block\n",
            "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
            "                blocks.append(block)     # add it the blocks list\n",
            "                block = {}               # re-init the block\n",
            "            block[\"type\"] = line[1:-1].rstrip()     \n",
            "        else:\n",
            "            key,value = line.split(\"=\") \n",
            "            block[key.rstrip()] = value.lstrip()\n",
            "    blocks.append(block)\n",
            "    \n",
            "    return blocks\n",
            "\n",
            "def prep_image(img, inp_dim):\n",
            "    \"\"\"\n",
            "    Prepare image for inputting to the neural network. Returns a tensor.\n",
            "    \"\"\"\n",
            "    # pylint: disable=no-member\n",
            "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
            "    img = letterbox_image(img, (inp_dim, inp_dim))\n",
            "    return torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
            "\n",
            "\n",
            "class EmptyLayer(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(EmptyLayer, self).__init__()\n",
            "        \n",
            "\n",
            "class DetectionLayer(nn.Module):\n",
            "    def __init__(self, anchors):\n",
            "        super(DetectionLayer, self).__init__()\n",
            "        self.anchors = anchors\n",
            "\n",
            "\n",
            "\n",
            "def create_modules(blocks):\n",
            "    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n",
            "    module_list = nn.ModuleList()\n",
            "    prev_filters = 3\n",
            "    output_filters = []\n",
            "    \n",
            "    for index, x in enumerate(blocks[1:]):\n",
            "        module = nn.Sequential()\n",
            "    \n",
            "        #check the type of block\n",
            "        #create a new module for the block\n",
            "        #append to module_list\n",
            "        \n",
            "        #If it's a convolutional layer\n",
            "        if (x[\"type\"] == \"convolutional\"):\n",
            "            #Get the info about the layer\n",
            "            activation = x[\"activation\"]\n",
            "            try:\n",
            "                batch_normalize = int(x[\"batch_normalize\"])\n",
            "                bias = False\n",
            "            except:\n",
            "                batch_normalize = 0\n",
            "                bias = True\n",
            "        \n",
            "            filters= int(x[\"filters\"])\n",
            "            padding = int(x[\"pad\"])\n",
            "            kernel_size = int(x[\"size\"])\n",
            "            stride = int(x[\"stride\"])\n",
            "        \n",
            "            if padding:\n",
            "                pad = (kernel_size - 1) // 2\n",
            "            else:\n",
            "                pad = 0\n",
            "        \n",
            "            #Add the convolutional layer\n",
            "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
            "            module.add_module(\"conv_{0}\".format(index), conv)\n",
            "        \n",
            "            #Add the Batch Norm Layer\n",
            "            if batch_normalize:\n",
            "                bn = nn.BatchNorm2d(filters)\n",
            "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
            "        \n",
            "            #Check the activation. \n",
            "            #It is either Linear or a Leaky ReLU for YOLO\n",
            "            if activation == \"leaky\":\n",
            "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
            "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
            "                \n",
            "            elif activation == \"mish\":\n",
            "                activn = Mish()\n",
            "                module.add_module(\"mish_{0}\".format(index), activn)\n",
            "\n",
            "        \n",
            "            #If it's an upsampling layer\n",
            "            #We use Bilinear2dUpsampling\n",
            "        elif (x[\"type\"] == \"upsample\"):\n",
            "            stride = int(x[\"stride\"])\n",
            "            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
            "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
            "                \n",
            "        #If it is a route layer\n",
            "        elif (x[\"type\"] == \"route\"):\n",
            "            x[\"layers\"] = x[\"layers\"].split(',')\n",
            "            filters = 0\n",
            "\n",
            "            for i in range(len(x[\"layers\"])):\n",
            "                pointer = int(x[\"layers\"][i])\n",
            "                if  pointer > 0:\n",
            "                    filters += output_filters[pointer]\n",
            "                else:\n",
            "                    filters += output_filters[index + pointer]\n",
            "\n",
            "            route = EmptyLayer()\n",
            "            module.add_module(\"route_{0}\".format(index), route)\n",
            "    \n",
            "        #shortcut corresponds to skip connection\n",
            "        elif x[\"type\"] == \"shortcut\":\n",
            "            shortcut = EmptyLayer()\n",
            "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
            "            \n",
            "        #Yolo is the detection layer\n",
            "        elif x[\"type\"] == \"yolo\":\n",
            "            mask = x[\"mask\"].split(\",\")\n",
            "            mask = [int(x) for x in mask]\n",
            "    \n",
            "            anchors = x[\"anchors\"].split(\",\")\n",
            "            anchors = [int(a) for a in anchors]\n",
            "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
            "            anchors = [anchors[i] for i in mask]\n",
            "    \n",
            "            detection = DetectionLayer(anchors)\n",
            "            module.add_module(\"Detection_{}\".format(index), detection)\n",
            "        \n",
            "        # Max pooling layer\n",
            "        elif x[\"type\"] == \"maxpool\":\n",
            "            stride = int(x[\"stride\"])\n",
            "            size = int(x[\"size\"])\n",
            "            max_pool = nn.MaxPool2d(size, stride, padding=size // 2)\n",
            "            module.add_module(\"maxpool_{}\".format(index), max_pool)\n",
            "                            \n",
            "        module_list.append(module)\n",
            "        prev_filters = filters\n",
            "        output_filters.append(filters)\n",
            "        \n",
            "    return (net_info, module_list)\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "class MyDarknet(nn.Module):\n",
            "    def __init__(self, config_file):\n",
            "        super(MyDarknet, self).__init__()\n",
            "        self.module_list = self.create_modules(config_file)\n",
            "\n",
            "    def forward(self, x):\n",
            "        outputs = []  # to store outputs for route layers\n",
            "        for i, module in enumerate(self.module_list):\n",
            "            module_type = module['type']\n",
            "\n",
            "            if module_type in ['convolutional', 'upsample', 'maxpool']:\n",
            "                x = module['module'](x)\n",
            "\n",
            "            elif module_type == 'route':\n",
            "                layers = module['layers']\n",
            "                maps = [outputs[i + layer] for layer in layers]\n",
            "                x = torch.cat(maps, 1)\n",
            "\n",
            "            outputs.append(x)\n",
            "\n",
            "        return x\n",
            "\n",
            "    def create_modules(self, config_file):\n",
            "        # Simplified: Parse configuration and create PyTorch modules\n",
            "        module_list = []\n",
            "        # Add parsing logic and initialize Mish/maxpool support here\n",
            "        return module_list\n",
            "\n",
            "\n",
            "    def load_weights(self, weightfile):\n",
            "        #Open the weights file\n",
            "        fp = open(weightfile, \"rb\")\n",
            "    \n",
            "        #The first 5 values are header information \n",
            "        # 1. Major version number\n",
            "        # 2. Minor Version Number\n",
            "        # 3. Subversion number \n",
            "        # 4,5. Images seen by the network (during training)\n",
            "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
            "        self.header = torch.from_numpy(header)\n",
            "        self.seen = self.header[3]   \n",
            "        \n",
            "        weights = np.fromfile(fp, dtype = np.float32)\n",
            "        \n",
            "        ptr = 0\n",
            "        for i in range(len(self.module_list)):\n",
            "            module_type = self.blocks[i + 1][\"type\"]\n",
            "    \n",
            "            #If module_type is convolutional load weights\n",
            "            #Otherwise ignore.\n",
            "            \n",
            "            if module_type == \"convolutional\":\n",
            "                model = self.module_list[i]\n",
            "                try:\n",
            "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
            "                except:\n",
            "                    batch_normalize = 0\n",
            "            \n",
            "                conv = model[0]\n",
            "                \n",
            "                \n",
            "                if (batch_normalize):\n",
            "                    bn = model[1]\n",
            "        \n",
            "                    #Get the number of weights of Batch Norm Layer\n",
            "                    num_bn_biases = bn.bias.numel()\n",
            "        \n",
            "                    #Load the weights\n",
            "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
            "                    ptr += num_bn_biases\n",
            "        \n",
            "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
            "                    ptr  += num_bn_biases\n",
            "        \n",
            "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
            "                    ptr  += num_bn_biases\n",
            "        \n",
            "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
            "                    ptr  += num_bn_biases\n",
            "        \n",
            "                    #Cast the loaded weights into dims of model weights. \n",
            "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
            "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
            "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
            "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
            "        \n",
            "                    #Copy the data to model\n",
            "                    bn.bias.data.copy_(bn_biases)\n",
            "                    bn.weight.data.copy_(bn_weights)\n",
            "                    bn.running_mean.copy_(bn_running_mean)\n",
            "                    bn.running_var.copy_(bn_running_var)\n",
            "                \n",
            "                else:\n",
            "                    #Number of biases\n",
            "                    num_biases = conv.bias.numel()\n",
            "                \n",
            "                    #Load the weights\n",
            "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
            "                    ptr = ptr + num_biases\n",
            "                \n",
            "                    #reshape the loaded weights according to the dims of the model weights\n",
            "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
            "                \n",
            "                    #Finally copy the data\n",
            "                    conv.bias.data.copy_(conv_biases)\n",
            "                    \n",
            "                #Let us load the weights for the Convolutional layers\n",
            "                num_weights = conv.weight.numel()\n",
            "                \n",
            "                #Do the same as above for weights\n",
            "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
            "                ptr = ptr + num_weights\n",
            "                \n",
            "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
            "                conv.weight.data.copy_(conv_weights)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "c82976f6-7272-461c-9953-02e486b45764",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "c82976f6-7272-461c-9953-02e486b45764",
        "outputId": "93a09008-2424-44d5-c079-6c4c8f2b672e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/COCO/annotations/instances_val2017.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-847663a6697a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiftyone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzoo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfoz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdarknet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMyDarknet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfull_train_yolov4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/full_train_yolov4.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                 \u001b[0;31m# annFile = path2json_train, transform=train_transform)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m val_dataset = Subset(CustomCoco(root = path2data_val,\n\u001b[0m\u001b[1;32m     71\u001b[0m                                 annFile = path2json_val, transform=eval_transform), list(range(0,20)))\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/COCO/annotations/instances_val2017.json'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from darknet import MyDarknet\n",
        "import full_train_yolov4\n",
        "import io\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a8cc17db-3614-4eff-a5e5-bad07dd4d164",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "a8cc17db-3614-4eff-a5e5-bad07dd4d164",
        "outputId": "3649da0a-c123-485f-ed97-4b1c620e2ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/COCO/annotations/instances_val2017.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-5e50f0dbc79c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfull_train_yolov4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/full_train_yolov4.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                 \u001b[0;31m# annFile = path2json_train, transform=train_transform)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m val_dataset = Subset(CustomCoco(root = path2data_val,\n\u001b[0m\u001b[1;32m     71\u001b[0m                                 annFile = path2json_val, transform=eval_transform), list(range(0,20)))\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/custom_coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/COCO/annotations/instances_val2017.json'"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import full_train_yolov4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4b6dba36-2037-4d11-b721-d9991c7f227c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "4b6dba36-2037-4d11-b721-d9991c7f227c",
        "outputId": "a49d252e-d22a-4c00-810b-2b0cff461c7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'full_train_yolov4' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-397b2e127122>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run YOLOv4 full training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting YOLOv4 training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfull_train_yolov4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_yolov4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'full_train_yolov4' is not defined"
          ]
        }
      ],
      "source": [
        "# Capture output for full training\n",
        "output_buffer_train = io.StringIO()\n",
        "sys.stdout = output_buffer_train\n",
        "\n",
        "# Run YOLOv4 full training\n",
        "print(\"Starting YOLOv4 training...\")\n",
        "full_train_yolov4.train_yolov4()\n",
        "print(\"Training completed successfully!\")\n",
        "\n",
        "# Reset stdout\n",
        "sys.stdout = sys.__stdout__\n",
        "\n",
        "# Print captured training output in the notebook\n",
        "train_output = output_buffer_train.getvalue()\n",
        "print(train_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d544167-5a57-4d5d-aca1-18266c0dd853",
      "metadata": {
        "id": "7d544167-5a57-4d5d-aca1-18266c0dd853"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}